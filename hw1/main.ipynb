{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dvclive import Live\n",
    "from optuna.trial import Trial\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>isHate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You should know women's sports are a joke</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You look like Sloth with deeper Down’s syndrome</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You look like Russian and speak like Indian. B...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women deserve to be abused, I guess.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women are made for making babies and cooking d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>From the midnight sun where the hot springs blow</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Don't say I'm not your type</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>And therefore never send to know for whom the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>And I can't stand another day</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>All values, unless otherwise stated, are in U...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  isHate\n",
       "0            You should know women's sports are a joke     1.0\n",
       "1      You look like Sloth with deeper Down’s syndrome     1.0\n",
       "2    You look like Russian and speak like Indian. B...     1.0\n",
       "3                 Women deserve to be abused, I guess.     1.0\n",
       "4    Women are made for making babies and cooking d...     1.0\n",
       "..                                                 ...     ...\n",
       "993   From the midnight sun where the hot springs blow     0.0\n",
       "994                        Don't say I'm not your type     0.0\n",
       "995   And therefore never send to know for whom the...     0.0\n",
       "996                      And I can't stand another day     0.0\n",
       "997   All values, unless otherwise stated, are in U...     0.0\n",
       "\n",
       "[998 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Ethos_Dataset_Binary.csv\", sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.98387097, 0.98360656, 0.97826087, 0.97333333,\n",
       "       0.96666667, 0.95454545, 0.94545455, 0.9375    , 0.90384615,\n",
       "       0.85714286, 0.8490566 , 0.84615385, 0.83333333, 0.82142857,\n",
       "       0.75      , 0.72222222, 0.67857143, 0.66666667, 0.60344828,\n",
       "       0.53061224, 0.5       , 0.4       , 0.33333333, 0.30232558,\n",
       "       0.296875  , 0.25      , 0.2       , 0.16666667, 0.16071429,\n",
       "       0.15254237, 0.11111111, 0.10344828, 0.09090909, 0.03896104,\n",
       "       0.03773585, 0.03174603, 0.03030303, 0.02985075, 0.02631579,\n",
       "       0.01886792, 0.01639344, 0.        ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isHate\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень хорошо видно, что, хоть датасет и позиционирует себя как датасет бинарной классификации хейтспича, значения `isHate` в нем небинарные :)<br>\n",
    "Возьмем трешхолд хейтспича = 0.5 и округлим значения больше трешхолда до 1, меньше - до 0, значения, которые равны трешхолду, случайным образом перемешаем между классами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "t_size = len(df[df['isHate'] == threshold])\n",
    "df.loc[df['isHate'] > threshold, \"isHate\"] = 1\n",
    "df.loc[df['isHate'] < threshold, \"isHate\"] = 0\n",
    "df.loc[df['isHate'] == threshold, \"isHate\"] = np.random.randint(0, 2, size=(t_size))\n",
    "df[\"isHate\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на тренировочную и тестовую выборки с помощью `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798, 200)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "df_train, df_test = train_test_split(\n",
    "    df, train_size=train_size, random_state=random_state\n",
    ")\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать `random forest`<br>\n",
    "В качестве фичей возьмем эмбеддинги текста на основе `tf-idf` векторизации\n",
    "\n",
    "Проверим, что модель обучается с фиксированным набором гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_train = df_train[\"comment\"].to_list()\n",
    "corpora_test = df_test[\"comment\"].to_list()\n",
    "y_train = df_train[\"isHate\"].to_numpy()\n",
    "y_test = df_test[\"isHate\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "emb_train = vectorizer.fit_transform(corpora_train).toarray()\n",
    "emb_test = vectorizer.transform(corpora_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(emb_train, y_train)\n",
    "forest.score(emb_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization & DVC Experiment Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора гиперпараметров будем использовать библиотеку `optuna`<br>\n",
    "Логгировать результаты экспериментов будем с помощью библиотеки `DVCLive`\n",
    "\n",
    "К сожалению, интеграция `DVCLive` c `optuna` [неполная](https://github.com/iterative/dvclive/issues/118#issuecomment-1285863519), и по умолчанию `DVCLive` сохраняет каждый отдельный запуск триала как отдельный эксперимент<br>\n",
    "Это не подходит под наши цели, так как всё логгирование осуществяется внутри гита и для сравнения метрик нам бы пришлось каждый раз менять различные коммиты экспериментов через `.git/refs`\n",
    "\n",
    "Изменим это поведение, обернув `objective` функцию внутри `Live` блока, таким образом мы будем сохранять значения гиперпараметров вместе с метриками на каждой итерации их поиска<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логгировать будем следующие сущности:\n",
    "- `n_embs_dim` - размерность эмбеддингов `tf-idf`\n",
    "- `n_estimators` - количество деревьев в лесу\n",
    "- `accuracy` - точность обученного классификатора на тестовой выборке\n",
    "\n",
    "Метрики (в нашем случае и гиперараметры) на каждой итерации триала логгируются в свой `.tsv` файл, расположенный в каталоге `experiments/dvc-optuna/plots/metrics`<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно сохраним 'статичные' гиперпараметры, такие как:\n",
    "- `seed` - начальное состояние ГПСЧ\n",
    "- `train_size` - размер тренировной выборки\n",
    "\n",
    "Данные значения логгируются в файл `experiments/dvc-optuna/params.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-11 16:17:41,941] A new study created in memory with name: no-name-b83d68f2-44e1-4d40-a1b3-b45edf776c6a\n",
      "[I 2023-06-11 16:17:43,320] Trial 0 finished with value: 0.695 and parameters: {'n_embs_dim': 350, 'n_estimators': 476, 'criterion': 'gini'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:43,425] Trial 1 finished with value: 0.65 and parameters: {'n_embs_dim': 204, 'n_estimators': 30, 'criterion': 'gini'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:44,732] Trial 2 finished with value: 0.66 and parameters: {'n_embs_dim': 113, 'n_estimators': 485, 'criterion': 'gini'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:45,192] Trial 3 finished with value: 0.65 and parameters: {'n_embs_dim': 222, 'n_estimators': 153, 'criterion': 'gini'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:45,507] Trial 4 finished with value: 0.69 and parameters: {'n_embs_dim': 509, 'n_estimators': 70, 'criterion': 'log_loss'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:45,995] Trial 5 finished with value: 0.67 and parameters: {'n_embs_dim': 625, 'n_estimators': 100, 'criterion': 'entropy'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:46,436] Trial 6 finished with value: 0.675 and parameters: {'n_embs_dim': 506, 'n_estimators': 86, 'criterion': 'log_loss'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:47,124] Trial 7 finished with value: 0.655 and parameters: {'n_embs_dim': 640, 'n_estimators': 153, 'criterion': 'entropy'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:47,969] Trial 8 finished with value: 0.66 and parameters: {'n_embs_dim': 181, 'n_estimators': 248, 'criterion': 'entropy'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:48,629] Trial 9 finished with value: 0.675 and parameters: {'n_embs_dim': 543, 'n_estimators': 156, 'criterion': 'entropy'}. Best is trial 0 with value: 0.695.\n",
      "[I 2023-06-11 16:17:50,143] Trial 10 finished with value: 0.7 and parameters: {'n_embs_dim': 350, 'n_estimators': 467, 'criterion': 'gini'}. Best is trial 10 with value: 0.7.\n",
      "[I 2023-06-11 16:17:51,716] Trial 11 finished with value: 0.675 and parameters: {'n_embs_dim': 332, 'n_estimators': 496, 'criterion': 'gini'}. Best is trial 10 with value: 0.7.\n",
      "[I 2023-06-11 16:17:53,032] Trial 12 finished with value: 0.705 and parameters: {'n_embs_dim': 375, 'n_estimators': 406, 'criterion': 'gini'}. Best is trial 12 with value: 0.705.\n",
      "[I 2023-06-11 16:17:54,229] Trial 13 finished with value: 0.685 and parameters: {'n_embs_dim': 353, 'n_estimators': 380, 'criterion': 'gini'}. Best is trial 12 with value: 0.705.\n",
      "[I 2023-06-11 16:17:55,483] Trial 14 finished with value: 0.71 and parameters: {'n_embs_dim': 414, 'n_estimators': 374, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:17:56,870] Trial 15 finished with value: 0.67 and parameters: {'n_embs_dim': 725, 'n_estimators': 358, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:17:58,254] Trial 16 finished with value: 0.685 and parameters: {'n_embs_dim': 417, 'n_estimators': 370, 'criterion': 'log_loss'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:17:59,260] Trial 17 finished with value: 0.69 and parameters: {'n_embs_dim': 434, 'n_estimators': 291, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:00,559] Trial 18 finished with value: 0.675 and parameters: {'n_embs_dim': 303, 'n_estimators': 423, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:01,586] Trial 19 finished with value: 0.655 and parameters: {'n_embs_dim': 279, 'n_estimators': 295, 'criterion': 'log_loss'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:02,731] Trial 20 finished with value: 0.69 and parameters: {'n_embs_dim': 588, 'n_estimators': 310, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:04,153] Trial 21 finished with value: 0.69 and parameters: {'n_embs_dim': 409, 'n_estimators': 431, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:05,599] Trial 22 finished with value: 0.68 and parameters: {'n_embs_dim': 465, 'n_estimators': 422, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:06,998] Trial 23 finished with value: 0.69 and parameters: {'n_embs_dim': 386, 'n_estimators': 431, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:08,005] Trial 24 finished with value: 0.68 and parameters: {'n_embs_dim': 259, 'n_estimators': 332, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:08,790] Trial 25 finished with value: 0.68 and parameters: {'n_embs_dim': 467, 'n_estimators': 216, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:10,134] Trial 26 finished with value: 0.7 and parameters: {'n_embs_dim': 371, 'n_estimators': 395, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:11,562] Trial 27 finished with value: 0.655 and parameters: {'n_embs_dim': 305, 'n_estimators': 458, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:12,753] Trial 28 finished with value: 0.66 and parameters: {'n_embs_dim': 241, 'n_estimators': 342, 'criterion': 'log_loss'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:14,313] Trial 29 finished with value: 0.69 and parameters: {'n_embs_dim': 338, 'n_estimators': 455, 'criterion': 'entropy'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:15,375] Trial 30 finished with value: 0.65 and parameters: {'n_embs_dim': 139, 'n_estimators': 402, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:16,664] Trial 31 finished with value: 0.68 and parameters: {'n_embs_dim': 379, 'n_estimators': 387, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:18,143] Trial 32 finished with value: 0.705 and parameters: {'n_embs_dim': 364, 'n_estimators': 471, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:19,592] Trial 33 finished with value: 0.675 and parameters: {'n_embs_dim': 312, 'n_estimators': 467, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:21,182] Trial 34 finished with value: 0.68 and parameters: {'n_embs_dim': 483, 'n_estimators': 483, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:22,654] Trial 35 finished with value: 0.67 and parameters: {'n_embs_dim': 444, 'n_estimators': 447, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:24,305] Trial 36 finished with value: 0.68 and parameters: {'n_embs_dim': 536, 'n_estimators': 480, 'criterion': 'gini'}. Best is trial 14 with value: 0.71.\n",
      "[I 2023-06-11 16:18:25,901] Trial 37 finished with value: 0.715 and parameters: {'n_embs_dim': 389, 'n_estimators': 497, 'criterion': 'gini'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:27,619] Trial 38 finished with value: 0.715 and parameters: {'n_embs_dim': 411, 'n_estimators': 497, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:29,349] Trial 39 finished with value: 0.695 and parameters: {'n_embs_dim': 405, 'n_estimators': 500, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:30,406] Trial 40 finished with value: 0.69 and parameters: {'n_embs_dim': 505, 'n_estimators': 259, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:30,501] Trial 41 finished with value: 0.62 and parameters: {'n_embs_dim': 435, 'n_estimators': 8, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:32,019] Trial 42 finished with value: 0.715 and parameters: {'n_embs_dim': 383, 'n_estimators': 416, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:33,522] Trial 43 finished with value: 0.7 and parameters: {'n_embs_dim': 402, 'n_estimators': 405, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:34,893] Trial 44 finished with value: 0.675 and parameters: {'n_embs_dim': 552, 'n_estimators': 351, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:36,322] Trial 45 finished with value: 0.655 and parameters: {'n_embs_dim': 197, 'n_estimators': 441, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:37,860] Trial 46 finished with value: 0.685 and parameters: {'n_embs_dim': 480, 'n_estimators': 414, 'criterion': 'entropy'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:39,487] Trial 47 finished with value: 0.645 and parameters: {'n_embs_dim': 284, 'n_estimators': 498, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:40,837] Trial 48 finished with value: 0.68 and parameters: {'n_embs_dim': 327, 'n_estimators': 371, 'criterion': 'log_loss'}. Best is trial 37 with value: 0.715.\n",
      "[I 2023-06-11 16:18:42,055] Trial 49 finished with value: 0.665 and parameters: {'n_embs_dim': 451, 'n_estimators': 324, 'criterion': 'entropy'}. Best is trial 37 with value: 0.715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_embs_dim': 389, 'n_estimators': 497, 'criterion': 'gini'} 0.715\n"
     ]
    }
   ],
   "source": [
    "with Live(\"experiments/dvc-optuna\", save_dvc_exp=True) as live:\n",
    "    def objective(trial: Trial):\n",
    "        n_embs_dim = trial.suggest_int(\"n_embs_dim\", 100, 768)\n",
    "        vectorizer = TfidfVectorizer(max_features=n_embs_dim)\n",
    "        emb_train = vectorizer.fit_transform(corpora_train).toarray()\n",
    "        emb_test = vectorizer.transform(corpora_test).toarray()\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "        forest = RandomForestClassifier(\n",
    "            criterion=criterion, n_estimators=n_estimators, random_state=random_state\n",
    "        )\n",
    "        forest.fit(emb_train, y_train)\n",
    "        score = forest.score(emb_test, y_test)\n",
    "        live.log_metric(\"n_embs_dim\", n_embs_dim)\n",
    "        live.log_metric(\"n_estimators\", n_estimators)\n",
    "        live.log_metric(\"accuracy\", score)\n",
    "        live.next_step()\n",
    "        return score\n",
    "    live.log_param(\"seed\", random_state)\n",
    "    live.log_param(\"train_size\", train_size)\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "best_params, best_value = study.best_params, study.best_value\n",
    "print(best_params, best_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова обучим модель, используя оптимальные гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.715"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=best_params[\"n_embs_dim\"])\n",
    "emb_train = vectorizer.fit_transform(corpora_train).toarray()\n",
    "emb_test = vectorizer.transform(corpora_test).toarray()\n",
    "forest = RandomForestClassifier(\n",
    "    criterion=best_params[\"criterion\"],\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    random_state=random_state\n",
    ")\n",
    "forest.fit(emb_train, y_train)\n",
    "forest.score(emb_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значения метрик для тестовый выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.698372</td>\n",
       "      <td>0.708605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.830645</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.678480</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.783270</td>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.683606</td>\n",
       "      <td>0.707525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.715</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0.0        1.0  accuracy   macro avg  weighted avg\n",
       "precision    0.741007   0.655738     0.715    0.698372      0.708605\n",
       "recall       0.830645   0.526316     0.715    0.678480      0.715000\n",
       "f1-score     0.783270   0.583942     0.715    0.683606      0.707525\n",
       "support    124.000000  76.000000     0.715  200.000000    200.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = forest.predict(emb_test)\n",
    "pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и для тренировочной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.995825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.997912</td>\n",
       "      <td>0.997504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993769</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.996885</td>\n",
       "      <td>0.997494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.997908</td>\n",
       "      <td>0.996875</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.997391</td>\n",
       "      <td>0.997492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>477.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>798.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0.0         1.0  accuracy   macro avg  weighted avg\n",
       "precision    0.995825    1.000000  0.997494    0.997912      0.997504\n",
       "recall       1.000000    0.993769  0.997494    0.996885      0.997494\n",
       "f1-score     0.997908    0.996875  0.997494    0.997391      0.997492\n",
       "support    477.000000  321.000000  0.997494  798.000000    798.000000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = forest.predict(emb_train)\n",
    "pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень хорошо видно, что случайный лес переобучился на тренировочных данных"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
